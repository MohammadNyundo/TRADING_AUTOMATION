{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammadNyundo/TRADING_AUTOMATION/blob/master/Fantasy_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OHxZEC3aiPX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9kr1iRLuW30",
        "outputId": "83f716b8-bbe6-439e-fbe4-3ee9951e1b20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ran the import statements.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Import Modules\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "import statistics\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# xgboost popular gradient boosting library for increase perfomance and speed\n",
        "import xgboost as xgb\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "nltk.download('punkt')  # Download the punkt tokenizer data\n",
        "\n",
        "\n",
        "# The following lines adjust the granularity of reporting.\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "# tf.keras.backend.set_floatx('float32')\n",
        "\n",
        "print(\"Ran the import statements.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-YQR85SukPY",
        "outputId": "0b7ca2c6-c162-4197-8fb0-c90f2f2c793e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n"
          ]
        }
      ],
      "source": [
        "#@title Print the version of tf for compatibility\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "atFY5WumJkyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29614afa-0be2-464f-8262-47ceb12a2f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql0QTKBwwoZm",
        "outputId": "efb8fdd6-a50b-4449-e720-05ea8f1a3e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Text  Label\n",
            "0  Dear Florida State Senator, Although many coul...      0\n",
            "1  Thousands of people around the world use cars....      0\n",
            "2  Although numerous amounts of people want to ba...      0\n",
            "3  There are many different things that you could...      0\n",
            "4  Dear Senator, The Electoral College as controv...      0\n",
            "                                                  Text  Label\n",
            "995    Pioneering Sustainable Urban Living  In an e...      1\n",
            "996    The Path to Sustainable Urban Living  In an ...      1\n",
            "997    A Paradigm Shift in Urban Living  In an era ...      1\n",
            "998    Revolutionizing Urban Living  In an age defi...      1\n",
            "999    Pioneering Sustainable Urban Living  In an a...      1\n"
          ]
        }
      ],
      "source": [
        "#@title Exploring the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/generated dataset.csv')\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "\n",
        "# 1 for AI generated and 0 for human generated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZi4E2Oo0-sc",
        "outputId": "a76f1cd7-4db2-4358-d586-9fc44b81623a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Text    1000 non-null   object\n",
            " 1   Label   1000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 15.8+ KB\n"
          ]
        }
      ],
      "source": [
        "#@title Basic info\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shrD3GXdAg3g",
        "outputId": "e686e7fd-eafb-4d35-8f22-352f09a621f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text     0\n",
            "Label    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#@title check for null values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# no null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KNtd1yenBZw2",
        "outputId": "fcacaffe-28a3-46e7-f249-ef5cb832943e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text  Label\n",
              "0  Dear Florida State Senator, Although many coul...      0\n",
              "1  Thousands of people around the world use cars....      0\n",
              "2  Although numerous amounts of people want to ba...      0\n",
              "3  There are many different things that you could...      0\n",
              "4  Dear Senator, The Electoral College as controv...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29a62968-c559-40b5-b53c-ef7a288e4652\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dear Florida State Senator, Although many coul...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Thousands of people around the world use cars....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Although numerous amounts of people want to ba...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>There are many different things that you could...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dear Senator, The Electoral College as controv...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29a62968-c559-40b5-b53c-ef7a288e4652')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29a62968-c559-40b5-b53c-ef7a288e4652 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29a62968-c559-40b5-b53c-ef7a288e4652');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-770b29e9-1619-4aca-8664-63e91b804fd3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-770b29e9-1619-4aca-8664-63e91b804fd3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-770b29e9-1619-4aca-8664-63e91b804fd3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_copy",
              "summary": "{\n  \"name\": \"df_copy\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 263,\n        \"samples\": [\n          \"Some of the advantages of limiting car usage is definitely that it would help not polute the air. With so many people driving cars in small towns that can quickly polute the air, especially in paris in source two where it says \\\"After days of nearrecord pollution, paris enforced a partial driving ban to clear the air of the global city\\\" With all of that pollution it can cause things such as global warming, and unhealthy air for our bodies. With just a small break of not driving the congestion was down to 60 percent in the capital of france, after five days of intensifying smog. Also with cold nights and warm days that also causes air to trap car emissions.\\n\\nAnother advantage of not driving cars, would be exercise. That would be such a good work out because you would have no other option except to walkie your bikes. You would get to socialize with everyone else who is walking to their jobs, or the grocery store all while getting a little bit of exercise with your family. With everyone walking and having nothing better to do, sports centers and parks activity has bloomed throughout the city, and new restaurants and upscale shopping districts have cropped up with sales.\\n\\nMore and more people have stopped getting carslicenses and instead taking the city bus, walking or riding their bike. A study last year found that driving by young people decreased 23 percent between 2001 and 2009. If more people would continue to either car pool or find a different way to get where they're going the worlds pollution would definitely begin to go down. Without all of the pollution that comes out of cars and that gets trap into the air, our air would begin to get so much more cleaner and safer for us to breathe.\\n\\nWith all of the money it takes to repair the roads, and the electricity it takes to run all of the stop lights, that money could go towards other things if people didnt drive as much as they do. Although many people need and use their cars for transportation theres always other ways to getting around town. It would also help save you money that is spent on gas every week, with the gas prices being unpredicatable every day.\\n\\nWith all of these advantages of not driving cars as much or at all, it would really help protect the worlds air, and could be a great source of exercise! Walking or riding your bike to wherever you need to go, or even just taking the city bus for two dollars. Also saving your money and spendind it on something else rather than wasting it on gas, or getting your car fixed. Or simply even buying a car, which can cost thousands of dollars.\",\n          \"What is the electoral college? The electoral college is a process which consists of the selection of the electors, meeting where they vote for president, and the counting of the votes. The real question is, is the electoral college helping us or is it just bringing our government farther towards disaster? The electoral college should be diminished because it is an unfair direct election, and the disaster factor.\\n\\nTo begin, by keeping the electoral college we could avoid run off elections. This would help our governmental system because as noted by Richard A. Posner, \\\"There is pressure for runoff elections when no candidate wins a majority of the votes cast.\\\" By keeping the electoral college, we could help resolve runoff elections. What we dont know is if it will permanently work for our governmental system and is it worth the risk? The electors are the ones voting for the president, so we should question whether they should have control to help avoid these runoff elections. The writer notes, \\\"... the pressure would greatly complicate the presidential election process, is reduced by electoral college...\\\" Posner. Although that is true, that is only one problem that would be resolved by the electoral college, and one benefit is outnumbered by the numerous disadvantages to the electoral college.\\n\\nMoreover, the disaster factor has a huge impact on the electoral collage. The writer states, \\\"The American people should consider themselves lucky that the 2000 fiasco was the biggest election crisis in a century the system allows for much worse\\\" Plumer. This disaster was not the first disaster to happen in the system. According to Plumer, the system has had much more effect and damage on the United States and this should not be repeated again. The writer also says, \\\"electors have occasionally refused to vote for their party's candidate and cast a deciding vote for whomever they please\\\" Plumer. This goes to show, not only is the electoral college unbeneficial, but they can be sneaky. The electoral college should be abolished because of the atrocious outcomes it is capable of doing to our government.\\n\\nNot only is the electoral college capable of the disaster factor, but it also has no direct election. As stated in \\\"What Is the Electoral College?\\\", \\\"... when you vote for your candidate you are acually voting for your candidate's electors.\\\" Office of the Federal Register. This means, voters have no control over whom controls the country. That is outrageous that the lives of these people will be affected by someone they did not want in the overpowering decisions of their country. Writer Plumer states, \\\"At the most basic level, the electoral college is unfair too voters\\\" as well as \\\"The electoral college is unfair, outdated, and irrational\\\" Plumer. Both of these point go to show that the electoral college is lowering our hopes for the government. The writer also notes, \\\"candidates don't spend time in states they have no chance in winning, focusing only on the tight races in the \\\"swing\\\" states\\\" Plumer. This proves that the electoral government thinks about their winning chances instead of our countries government. Are we going to sit here and let them take advantage of their power that will lead us towards disaster?\\n\\nAll in all, the electoral collage should be banished from the government.\",\n          \"  A Vision of Urban Sustainability  In our rapidly urbanizing world, the concept of car-free cities is gaining traction as an innovative approach to address the challenges posed by urbanization, environmental degradation, and the need for sustainable living. These cities propose a transformative shift where private automobiles are either severely restricted or entirely absent, promoting sustainable transportation alternatives and greener urban environments. This essay explores the idea of car-free cities, highlighting their potential benefits, challenges, and strategies for successful implementation.  Car-free cities offer a visionary perspective on urban living that prioritizes sustainability, health, and community well-being.  Environmental Sustainability: Car-free cities play a pivotal role in addressing climate change by significantly reducing air pollution and greenhouse gas emissions. This shift toward a more sustainable urban lifestyle is essential in the battle against global warming.  Public Health: These cities emphasize pedestrian and cyclist safety, leading to fewer accidents and improved public health. Reduced air pollution levels contribute to better respiratory and cardiovascular well-being among urban residents.  Optimal Use of Urban Space: Car-free cities efficiently repurpose urban space by transforming parking lots and wide roads into green parks, recreational areas, and pedestrian zones. This reconfiguration revitalizes the urban landscape, enhancing the overall quality of life.  Traffic Congestion Reduction: A reduction in private cars results in less traffic congestion, shorter commutes, and a more stress-free daily urban experience for residents and commuters.  Economic Benefits: Car ownership can be a costly endeavor. By depending less on private vehicles, residents have the opportunity to save money on car-related expenses, leading to improved financial stability.  Transitioning to car-free cities, however, poses a unique set of challenges:  Resistance to Change: Many individuals heavily rely on their cars for daily activities and commutes, leading to potential resistance to the transition due to concerns about convenience and a perceived loss of personal freedom.  Public Transportation Infrastructure: The success of car-free cities depends on the availability and efficiency of public transportation. Substantial investments are needed to expand and enhance public transit systems, providing convenient alternatives to private cars.  Economic Impact: Businesses that rely on car-dependent customers may experience declining revenues during the transition. Addressing this economic impact and supporting affected businesses is a vital aspect of the transition.  Urban Planning and Infrastructure: The reconfiguration of urban areas for car-free living requires comprehensive planning and substantial investments in infrastructure, a complex and time-consuming process.  Solutions for successful implementation of car-free cities include expanding and enhancing public transportation, promoting active transportation methods such as walking and cycling, encouraging carpooling and ride-sharing, adopting electric and sustainable transportation options, and focusing on public education and awareness.  In conclusion, car-free cities offer an inspiring vision of a more sustainable, healthier, and community-centric urban future. Despite the challenges, the potential benefits in terms of environmental sustainability, public health, efficient urban living, and cost savings make them a worthwhile goal. By investing in public transportation, promoting alternative transportation methods, and engaging the public in the planning process, cities can work toward a future where private cars are no longer the dominant mode of transportation. Car-free cities represent a hopeful glimpse of a more sustainable and people-centric urban environment.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#@ creating a copy of the dataset\n",
        "df_copy = df.copy()\n",
        "df_copy.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKZ_82JQFumj",
        "outputId": "70f0e938-73e7-4245-9c17-9445d05db7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "500\n"
          ]
        }
      ],
      "source": [
        "#@title Balancing the dataset\n",
        "df_label_human = df_copy[df_copy['Label'] == 0]\n",
        "df_label_AI = df_copy[df_copy['Label'] == 1]\n",
        "\n",
        "print(len(df_label_human))\n",
        "print(len(df_label_AI))\n",
        "\n",
        "# our dataset is balanced with 500 human examples and 500 AI examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoe9KQrcHvaC",
        "outputId": "ef4e828a-1739-4518-861c-e34abc525181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day people wake ready work school way vary people drive cars s change helping environment relaxed advantages limiting car usage reducing car usage emmitions produced automobiles reduced help environment cars release gas air causes smog traps gases inhibits able escape atmosphere largely congested cities smog harmful 147 micrograms particulate matter seen paris france paragraph 17 paris enforced partial driving ban smog just day reduced driving smog clears didn t enforce ban day paragraph 19 passenger cars responsible 12 percent greenhouse gas emissions europe 50 percent united states paragraph 5 staggering statistic open eyes cars really effect environment just week didn t use car reduce greenhouse gases drastically united states today recent studies shown americans buying fewer cars driving paragraph 29 1995 number miles driven person dropped 9 percent paragraph 32 lesser people buying driving new cars gas emissions change drastically years help evrioment exonentialy drive car places day relaxed person vauban germany residents given cars 30 percent residents owning cars suburban town comman mothers minivan ship children forth town heidrun walter feels wise given automobile town car tense m happier way walter states paragraph 3 past years bogota colombia day year taxis buses permitted day cars dark clouds circled city rain splattered heads people didn t revert cars businessman carlos arturo plaza states s good opportunity away stress lower air pollution paragraph 24 day popular cities columbia joined celebrate day people use cars people need meet people order work hangout friends anymore homes using technology access friends continue work paragraph 35 having commute office people feel stressed relaxed able work comfort home people used accossicate cars best way place b thinking consequences planet step look bigger picture  \n",
            "\n",
            "  A Sustainable Urban Future  Car-free cities are emerging as a powerful response to the pressing challenges of urbanization. These cities aspire to create environments where private automobiles are either severely restricted or completely banned, emphasizing sustainable transportation alternatives, cleaner air, and vibrant urban living. This essay delves into the concept of car-free cities, exploring their potential benefits, challenges, and solutions.  Car-free cities are gaining momentum as a solution to pressing urban challenges. These cities aim to create environments where private automobiles are either restricted or entirely eliminated in favor of sustainable transportation alternatives. This essay explores the concept of car-free cities, emphasizing their potential benefits, challenges, and solutions.  Car-free cities represent a vision for urban living that emphasizes sustainability, health, and efficiency:  Environmental Sustainability: A reduced reliance on cars leads to decreased air pollution and greenhouse gas emissions. This shift towards environmental sustainability helps mitigate climate change and promotes cleaner, healthier cities.  Public Health: Car-free cities create safer environments for pedestrians and cyclists. Reduced air pollution levels contribute to better respiratory and cardiovascular health, enhancing overall public well-being.  Optimal Space Utilization: Car-free cities make efficient use of urban space. Parking lots and wide roads can be repurposed into green spaces, parks, and pedestrian zones, enhancing the quality of life.  Traffic Congestion Reduction: Fewer private cars on the road result in less traffic congestion, shorter commutes, and lower stress levels for city residents and commuters.  Economic Savings: Car ownership can be expensive, including vehicle purchase, fuel, and insurance. Car-free cities offer residents an opportunity to save money and improve their financial stability.  Despite their potential, transitioning to car-free cities is not without its challenges:  Resistance to Change: Many individuals rely heavily on cars for daily activities and commutes. The shift to car-free living can face resistance from those who fear inconvenience and a loss of personal autonomy.  Public Transportation Infrastructure: A robust public transportation system is vital for the success of car-free cities. Investments in public transit and its expansion are crucial to provide convenient alternatives to private cars.  Economic Impact: Businesses that depend on car-dependent customers may experience revenue declines during the transition. Addressing this economic impact and supporting affected businesses is essential.  Urban Planning and Infrastructure: Redesigning urban areas for car-free living requires comprehensive planning and substantial infrastructure investments, which can be complex and time-consuming.  Solutions for the successful adoption of car-free cities include expanding public transportation networks, promoting active transportation such as walking and cycling, encouraging carpooling and ride-sharing, adopting electric and sustainable transportation options, and focusing on public education and awareness.  In conclusion, car-free cities present a promising vision for a more sustainable and healthy urban future. While they come with challenges, the potential benefits in terms of environmental sustainability, public health, efficient urban living, and cost savings make them a compelling objective. By investing in public transportation, promoting alternative transportation methods, and involving the public in the planning process, cities can work towards a future where private cars no longer dominate the urban landscape. Car-free cities offer a hopeful glimpse of a more sustainable and vibrant urban environment. \n",
            "\n",
            "0 \n",
            "\n",
            "1000 \t 1000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "#@title converting the column 'Text' and 'Label' of D.F to lists\n",
        "\n",
        "text = df_copy.iloc[100]['Text']\n",
        "texts = df_copy['Text'].tolist()\n",
        "labels = df_copy['Label'].tolist()\n",
        "\n",
        "# processing the text\n",
        "# Preprocess the input text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = re.split(r'\\W+', text.lower())\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in ENGLISH_STOP_WORDS]\n",
        "    # Rejoin the tokens into a single string\n",
        "    processed_text = ' '.join(tokens)\n",
        "    return processed_text\n",
        "\n",
        "text1 = preprocess_text(text)\n",
        "\n",
        "\n",
        "#printing to inspect the data\n",
        "print(text1, '\\n')\n",
        "print(texts[-110], '\\n')\n",
        "print(labels[15], '\\n')\n",
        "print(len(texts), '\\t', len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mIF7qL7Jy6C",
        "outputId": "6b282c78-0510-45e0-8a58-8a084546a2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "#@title Number of Sentences\n",
        "def count_sentences_total(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    total_sentence_count = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        sentences = re.split(r'[.?!+]', paragraph)\n",
        "        sentences = [s for s in sentences if s.strip()]\n",
        "        total_sentence_count += len(sentences)\n",
        "\n",
        "    return total_sentence_count\n",
        "\n",
        "total_sentence_count = count_sentences_total(text1)\n",
        "print(total_sentence_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTThkECI0rgy",
        "outputId": "4dcc86c4-cac5-4f2a-c336-c76846013733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "277\n"
          ]
        }
      ],
      "source": [
        "#@title Counting number of words\n",
        "def count_total_words(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    total_word_count = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        sentences = re.split(r'[.?!+]', paragraph)\n",
        "        for sentence in sentences:\n",
        "            words = sentence.split()\n",
        "            total_word_count += len(words)\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "total_word_count = count_total_words(text1)\n",
        "print(total_word_count)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVaqShJH1CQj",
        "outputId": "c40e0f1f-5b7f-4f2b-c8f0-5fd6caf2b9a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "#@title Total number of parantheses\n",
        "def detect_total_parantheses(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    total_parantheses_presence = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if '(' in paragraph or ')' in paragraph:\n",
        "            total_parantheses_presence += 1\n",
        "\n",
        "    return total_parantheses_presence\n",
        "\n",
        "total_parantheses_presence = detect_total_parantheses(text1)\n",
        "print(total_parantheses_presence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAoKbsgM1enA",
        "outputId": "99dd585b-4aba-450a-a0e7-01f63b45b14c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "#@title Number of dashes\n",
        "def detect_total_dash(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    total_dash_presence = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if '-' in paragraph:\n",
        "            total_dash_presence += 1\n",
        "\n",
        "    return total_dash_presence\n",
        "\n",
        "total_dash_presence = detect_total_dash(text1)\n",
        "print(total_dash_presence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-qkw7t_2YA9",
        "outputId": "82d4a4b7-9f08-4554-9953-e202625c1f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "#@title Total number of colons and semicolons\n",
        "def detect_total_semicolon_colon(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    total_punctuation_presence = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if ';' in paragraph or ':' in paragraph:\n",
        "            total_punctuation_presence += 1\n",
        "\n",
        "    return total_punctuation_presence\n",
        "\n",
        "total_semicolon_colon_presence = detect_total_semicolon_colon(text1)\n",
        "print(total_semicolon_colon_presence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T9e8Ggm_F-W",
        "outputId": "7a22efba-2f5e-4662-b2ad-9928c1b9d8c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "#@title Total Number of question marks\n",
        "def detect_total_question_mark(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    total_question_mark_presence = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if '?' in paragraph:\n",
        "            total_question_mark_presence += 1\n",
        "\n",
        "    return total_question_mark_presence\n",
        "\n",
        "total_question_mark_presence = detect_total_question_mark(text1)\n",
        "print(total_question_mark_presence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKfkq99UFciU",
        "outputId": "280b6940-0455-4eeb-bff3-4d284261b289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "#@title Total number of Apostrophes\n",
        "def detect_total_apostrophe(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    total_apostrophe_presence = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if \"'\" in paragraph:\n",
        "            total_apostrophe_presence += 1\n",
        "\n",
        "    return total_apostrophe_presence\n",
        "\n",
        "total_apostrophe_presence = detect_total_apostrophe(text1)\n",
        "print(total_apostrophe_presence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipTfWqoKGxPz",
        "outputId": "6fbb9248-2c5c-41d2-e175-9955ebf60cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "#@title Calculating the standard deviation of sentence lenghts entire text\n",
        "def total_std_dev_sentence_length(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    all_sentence_lengths = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        sentences = re.split(r'[.?!]+', paragraph)\n",
        "        sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence.strip()]\n",
        "        all_sentence_lengths.extend(sentence_lengths)\n",
        "\n",
        "    if len(all_sentence_lengths) > 1:\n",
        "        total_std_dev = statistics.stdev(all_sentence_lengths)\n",
        "        total_std_dev = round(total_std_dev, 2)\n",
        "    else:\n",
        "        total_std_dev = 0  # if not enough sentences\n",
        "\n",
        "    return total_std_dev\n",
        "\n",
        "total_std_dev = total_std_dev_sentence_length(text1)\n",
        "print(total_std_dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OyH_jO5M_np",
        "outputId": "1b8ee4ed-60de-4018-ed55-35a5a30f0ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "#@title Calculating the difference in mean sentence lenght per consequtive paragraphs\n",
        "def total_mean_diff_sentence_length(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    all_sentence_lengths = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        sentences = re.split(r'[.?!]+', paragraph)\n",
        "        sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence.strip()]\n",
        "        all_sentence_lengths.extend(sentence_lengths)\n",
        "\n",
        "    differences = [abs(all_sentence_lengths[i] - all_sentence_lengths[i+1]) for i in range(len(all_sentence_lengths) - 1)]\n",
        "\n",
        "    if differences:\n",
        "        total_mean_diff = statistics.mean(differences)\n",
        "        total_mean_diff = round(total_mean_diff, 2)\n",
        "    else:\n",
        "        total_mean_diff = 0  # handle where no consecutive sentences or only empty sentences are present.\n",
        "\n",
        "    return total_mean_diff\n",
        "\n",
        "total_mean_diff = total_mean_diff_sentence_length(text1)\n",
        "print(total_mean_diff)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXaK5NggOGJw",
        "outputId": "aa7060db-18bb-4a15-838c-d3025c85922b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "#@title detecting presence of short Sentences\n",
        "def detect_total_short_sentences(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    has_short_sentence = any(any(len(sentence.split()) < 11 for sentence in re.split(r'[.?!]+', paragraph) if sentence.strip()) for paragraph in paragraphs)\n",
        "\n",
        "    return 1 if has_short_sentence else 0\n",
        "\n",
        "total_short_sentence_presence = detect_total_short_sentences(text1)\n",
        "print(total_short_sentence_presence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq6silH-PBov",
        "outputId": "6c2ea003-232c-4f29-8afe-353ae628ba70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "#@title Detecting long sentences in the paragraph\n",
        "def detect_total_long_sentences(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    has_long_sentence = any(any(len(sentence.split()) > 34 for sentence in re.split(r'[.?!]+', paragraph) if sentence.strip()) for paragraph in paragraphs)\n",
        "\n",
        "    return 1 if has_long_sentence else 0\n",
        "\n",
        "total_long_sentence_presence = detect_total_long_sentences(text1)\n",
        "print(total_long_sentence_presence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeeYelM_Y3Hr",
        "outputId": "f2b795f4-dc63-4388-cb67-4266b9afda71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "#@title Checking if there is a number in the entire text\n",
        "def detect_total_numbers(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    has_number = any(any(char.isdigit() for char in paragraph) for paragraph in paragraphs)\n",
        "\n",
        "    return 1 if has_number else 0\n",
        "\n",
        "total_number_presence = detect_total_numbers(text1)\n",
        "print(total_number_presence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnZmsyYCj7Pa",
        "outputId": "5f9f30bc-3d82-41b3-b10f-bffacea41381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "#@title Checks the ratio of capital letters to periods\n",
        "def detect_total_capital_vs_periods(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    has_condition = any(sum(1 for char in paragraph if char.isupper()) >= 2 * paragraph.count('.') for paragraph in paragraphs)\n",
        "\n",
        "    return 1 if has_condition else 0\n",
        "\n",
        "total_capital_vs_period_presence = detect_total_capital_vs_periods(text1)\n",
        "print(total_capital_vs_period_presence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McGtLWV3niyg",
        "outputId": "bb9c2bca-a8ed-4ed0-b68a-b1fbd535294d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['day', 'people', 'wake', 'ready', 'work', 'school', 'way', 'vary', 'people', 'drive', 'cars', 's', 'change', 'helping', 'environment', 'relaxed', 'advantages', 'limiting', 'car', 'usage', 'reducing', 'car', 'usage', 'emmitions', 'produced', 'automobiles', 'reduced', 'help', 'environment', 'cars', 'release', 'gas', 'air', 'causes', 'smog', 'traps', 'gases', 'inhibits', 'able', 'escape', 'atmosphere', 'largely', 'congested', 'cities', 'smog', 'harmful', '147', 'micrograms', 'particulate', 'matter', 'seen', 'paris', 'france', 'paragraph', '17', 'paris', 'enforced', 'partial', 'driving', 'ban', 'smog', 'just', 'day', 'reduced', 'driving', 'smog', 'clears', 'didn', 't', 'enforce', 'ban', 'day', 'paragraph', '19', 'passenger', 'cars', 'responsible', '12', 'percent', 'greenhouse', 'gas', 'emissions', 'europe', '50', 'percent', 'united', 'states', 'paragraph', '5', 'staggering', 'statistic', 'open', 'eyes', 'cars', 'really', 'effect', 'environment', 'just', 'week', 'didn', 't', 'use', 'car', 'reduce', 'greenhouse', 'gases', 'drastically', 'united', 'states', 'today', 'recent', 'studies', 'shown', 'americans', 'buying', 'fewer', 'cars', 'driving', 'paragraph', '29', '1995', 'number', 'miles', 'driven', 'person', 'dropped', '9', 'percent', 'paragraph', '32', 'lesser', 'people', 'buying', 'driving', 'new', 'cars', 'gas', 'emissions', 'change', 'drastically', 'years', 'help', 'evrioment', 'exonentialy', 'drive', 'car', 'places', 'day', 'relaxed', 'person', 'vauban', 'germany', 'residents', 'given', 'cars', '30', 'percent', 'residents', 'owning', 'cars', 'suburban', 'town', 'comman', 'mothers', 'minivan', 'ship', 'children', 'forth', 'town', 'heidrun', 'walter', 'feels', 'wise', 'given', 'automobile', 'town', 'car', 'tense', 'm', 'happier', 'way', 'walter', 'states', 'paragraph', '3', 'past', 'years', 'bogota', 'colombia', 'day', 'year', 'taxis', 'buses', 'permitted', 'day', 'cars', 'dark', 'clouds', 'circled', 'city', 'rain', 'splattered', 'heads', 'people', 'didn', 't', 'revert', 'cars', 'businessman', 'carlos', 'arturo', 'plaza', 'states', 's', 'good', 'opportunity', 'away', 'stress', 'lower', 'air', 'pollution', 'paragraph', '24', 'day', 'popular', 'cities', 'columbia', 'joined', 'celebrate', 'day', 'people', 'use', 'cars', 'people', 'need', 'meet', 'people', 'order', 'work', 'hangout', 'friends', 'anymore', 'homes', 'using', 'technology', 'access', 'friends', 'continue', 'work', 'paragraph', '35', 'having', 'commute', 'office', 'people', 'feel', 'stressed', 'relaxed', 'able', 'work', 'comfort', 'home', 'people', 'used', 'accossicate', 'cars', 'best', 'way', 'place', 'b', 'thinking', 'consequences', 'planet', 'step', 'look', 'bigger', 'picture']]]\n"
          ]
        }
      ],
      "source": [
        "#@title Performimg sentence tokenization\n",
        "def tokenize_text(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
        "\n",
        "    # Tokenize words in each sentence of each paragraph\n",
        "    tokenized_text = []\n",
        "    for paragraph in paragraphs:\n",
        "        sentences = re.split(r'[.?!+]', paragraph)\n",
        "        sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "        tokenized_sentences = []\n",
        "        for sentence in sentences:\n",
        "            words = sentence.split()\n",
        "            words = [w.strip() for w in words if w.strip()]\n",
        "            tokenized_sentences.append(words)\n",
        "\n",
        "        tokenized_text.append(tokenized_sentences)\n",
        "\n",
        "    return tokenized_text\n",
        "\n",
        "tokenized_text = tokenize_text(text1)\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-JayFYVo81K",
        "outputId": "166019df-b5dc-431a-c5a5-4c24dc1194db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "#@ title Analyzing the sentiment of the entire text\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download VADER lexicon.\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    # Initialize VADER sentiment analyzer\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Calculate compound sentiment score for the entire text\n",
        "    sentiment_score = sia.polarity_scores(text)['compound']\n",
        "\n",
        "    # Classify sentiment based on the compound score\n",
        "    if sentiment_score >= 0.05:\n",
        "        sentiment = 1 # for positive\n",
        "    elif sentiment_score <= -0.05:\n",
        "        sentiment = -1 # for negative\n",
        "    else:\n",
        "        sentiment = 0\n",
        "\n",
        "    return sentiment\n",
        "\n",
        "sentiment = analyze_sentiment(text1)\n",
        "print(sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "X32yRiugTSM0",
        "outputId": "0f770ecc-b3d0-4ac0-a16d-2d6e32fb29c0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-ed493de70e02>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mrepetitive_language\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_repetitive_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepetitive_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-ed493de70e02>\u001b[0m in \u001b[0;36mdetect_repetitive_language\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetect_repetitive_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mfreq_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Calculate the frequency of the most common words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "def detect_repetitive_language(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    freq_dist = nltk.FreqDist(tokens)\n",
        "\n",
        "    # Calculate the frequency of the most common words\n",
        "    most_common_words = sorted(freq_dist, key=freq_dist.get, reverse=True)[:10]\n",
        "    num_common_words = sum(freq_dist[word] > 5 for word in most_common_words)\n",
        "\n",
        "    if num_common_words > 5:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "repetitive_language = detect_repetitive_language(text1)\n",
        "print(repetitive_language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnfzEpz7AK1e"
      },
      "outputs": [],
      "source": [
        "#@title Checking for specific words\n",
        "def detect_word_although_total(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    has_word_although = any('although' in paragraph.lower() for paragraph in paragraphs)\n",
        "    return 1 if has_word_although else 0\n",
        "\n",
        "def detect_word_however_total(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    has_word_however = any('however' in paragraph.lower() for paragraph in paragraphs)\n",
        "    return 1 if has_word_however else 0\n",
        "\n",
        "def detect_word_but_total(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    has_word_but = any('but' in paragraph.lower() for paragraph in paragraphs)\n",
        "    return 1 if has_word_but else 0\n",
        "\n",
        "def detect_word_because_total(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    has_word_because = any('because' in paragraph.lower() for paragraph in paragraphs)\n",
        "    return 1 if has_word_because else 0\n",
        "\n",
        "def detect_word_this_total(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    has_word_this = any('this' in paragraph.lower() for paragraph in paragraphs)\n",
        "    return 1 if has_word_this else 0\n",
        "\n",
        "def detect_word_others_researchers_total(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    has_word_others_researchers = any('others' in paragraph.lower() or 'researchers' in paragraph.lower() for paragraph in paragraphs)\n",
        "    return 1 if has_word_others_researchers else 0\n",
        "\n",
        "def detect_word_etc_total(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    has_word_etc = any('etc.' in paragraph.lower() for paragraph in paragraphs)\n",
        "    return 1 if has_word_etc else 0\n",
        "\n",
        "def detect_word_in_conclusion_total(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    has_word_in_conclusion = any('conclusion' in paragraph.lower() for paragraph in paragraphs)\n",
        "    return 1 if has_word_in_conclusion else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA8cO2IOeY-t"
      },
      "outputs": [],
      "source": [
        "# Preprocess the input text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = re.split(r'\\W+', text.lower())\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in ENGLISH_STOP_WORDS]\n",
        "    # Rejoin the tokens into a single string\n",
        "    processed_text = ' '.join(tokens)\n",
        "    return processed_text\n",
        "\n",
        "preprocessed_texts = []\n",
        "for text in texts:\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "    preprocessed_texts.append(preprocessed_text)\n",
        "\n",
        "# Loop through the preprocessed texts\n",
        "results = []\n",
        "for text in tqdm(preprocessed_texts):\n",
        "    result = {\n",
        "        'total_number_sentences': count_sentences_total(text),\n",
        "        'total_number_words': count_total_words(text),\n",
        "        'total_number_parantheses': detect_total_parantheses(text),\n",
        "        'total_number_dash': detect_total_dash(text),\n",
        "        'total_colons': detect_total_semicolon_colon(text),\n",
        "        'total_number_questionmark': detect_total_question_mark(text),\n",
        "        'total_number_apostrophe': detect_total_apostrophe(text),\n",
        "        'std_dev_sentence_length': total_std_dev_sentence_length(text),\n",
        "        'mean_diff_consecutive_sentence': total_mean_diff_sentence_length(text),\n",
        "        'short_sentence_presence': detect_total_short_sentences(text),\n",
        "        'long_sentence_presence': detect_total_long_sentences(text),\n",
        "        'numbers_presence': detect_total_numbers(text),\n",
        "        'capital_vs_period_presence': detect_total_capital_vs_periods(text),\n",
        "        'word_although_presence': detect_word_although_total(text),\n",
        "        'word_however_presence': detect_word_however_total(text),\n",
        "        'word_but_presence': detect_word_but_total(text),\n",
        "        'word_this_presence': detect_word_this_total(text),\n",
        "        'word_others_researchers_presence': detect_word_others_researchers_total(text),\n",
        "        'word_et_presence': detect_word_etc_total(text),\n",
        "        'word_inconclusion_presence': detect_word_in_conclusion_total(text),\n",
        "        'repetitive_words': detect_repetitive_language(text),\n",
        "        'Sentiment': analyze_sentiment(text),\n",
        "        # 'label': df_copy['Label']\n",
        "    }\n",
        "    results.append(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAtotXKtnh7l"
      },
      "outputs": [],
      "source": [
        "#@title Converting the results to a dataFrame to use as feature\n",
        "dataFrame_features = pd.DataFrame(results)\n",
        "print(len(dataFrame_features))\n",
        "dataFrame_features ['LABEL']= df_copy['Label']\n",
        "dataFrame_features.head()\n",
        "dataFrame_features.tail()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UQmta-4pO6h"
      },
      "outputs": [],
      "source": [
        "#@title Perfoming correlation to see what features can be used to best train the model\n",
        "# a correaltion analysis was performed and if a feature had a correlation of more\n",
        "# than o.5 to the label it was used as a feature to train the model.\n",
        "df_correlated = dataFrame_features.corr()\n",
        "#print(df_correlated, '\\n')\n",
        "df_correlated.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFUZZs527O9h"
      },
      "outputs": [],
      "source": [
        "correlation_matrix = dataFrame_features ['numbers_presence'].corr(dataFrame_features['LABEL'])\n",
        "print(correlation_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0vOfoUdEZSo"
      },
      "source": [
        "*italicized text*\n",
        "# we will use:\n",
        "# total_number_words, numbers_presence, word_but_presence, word_inconclusion_presence,\n",
        "# repetitive_words and Sentiment\n",
        "\n",
        "#The correlation threshold used was 0.5/-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5qrXmD0EsyY"
      },
      "outputs": [],
      "source": [
        "#@title splitting the dataFrame_features into train and test data\n",
        "train_df = dataFrame_features.sample(frac=0.9, random_state=42)\n",
        "test_df= dataFrame_features.drop(train_df.index)\n",
        "\n",
        "# shuffle the training set to avoid overfitting and prevent bias introduced by sequency of the data\n",
        "train_df = train_df.reindex(np.random.permutation(train_df.index))\n",
        "\n",
        "train_df.head()\n",
        "test_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeMy_WNW_Aoo"
      },
      "outputs": [],
      "source": [
        "# @title Calulating the z-scores in the train set\n",
        "# Calculate the z-scores of each column in the training set and\n",
        "# write those z-scores in a new pandas DataFrame named train_df_norm.\n",
        "label_column = 'LABEL'\n",
        "\n",
        "# Select columns for normalization (excluding the label column)\n",
        "columns_to_normalize = train_df.columns[train_df.columns != label_column]\n",
        "\n",
        "# Calculate mean and standard deviation for the selected columns\n",
        "train_df_mean = train_df[columns_to_normalize].mean()\n",
        "train_df_std = train_df[columns_to_normalize].std()\n",
        "\n",
        "column1= train_df[['total_number_words', 'numbers_presence', 'word_but_presence', 'word_inconclusion_presence', 'repetitive_words', 'Sentiment']].std()\n",
        "print (column1)\n",
        "# Calculate z-scores for the selected columns\n",
        "train_df_norm = (train_df[columns_to_normalize] - train_df_mean) / train_df_std\n",
        "\n",
        "# Combine the normalized columns with the non-normalized ones, including the label\n",
        "train_df_norm_with_label = pd.concat([train_df[label_column], train_df_norm], axis=1)\n",
        "\n",
        "# Examine some of the values of the normalized training set\n",
        "print(\"Standard DEV \\n\", train_df_std, \"\\n\")\n",
        "print(\"Mean \\n\", train_df_mean, \"\\n\")\n",
        "train_df_norm_with_label.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKgVuiBkASkh"
      },
      "outputs": [],
      "source": [
        "# Assuming 'label_column' is the column you want to exclude from normalization\n",
        "label_column = 'LABEL'\n",
        "\n",
        "# Select columns for normalization (excluding the label column)\n",
        "columns_to_normalize_test = test_df.columns[test_df.columns != label_column]\n",
        "\n",
        "# Calculate z-scores for the selected columns in the test set\n",
        "test_df_norm = (test_df[columns_to_normalize_test] - train_df_mean[columns_to_normalize_test]) / train_df_std[columns_to_normalize_test]\n",
        "\n",
        "# Combine the normalized columns with the non-normalized ones, including the label\n",
        "test_df_norm_with_label = pd.concat([test_df[label_column], test_df_norm], axis=1)\n",
        "\n",
        "# Examine some of the values of the normalized test set\n",
        "test_df_norm_with_label.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goq5iuRsAn5q"
      },
      "outputs": [],
      "source": [
        "#@title Represent features as input layer\n",
        "inputs = {\n",
        "    # Features used to train the model on.\n",
        "      'total_number_words': tf.keras.Input(shape=(1,)),\n",
        "      'numbers_presence': tf.keras.Input(shape=(1,)),\n",
        "      'word_but_presence': tf.keras.Input(shape=(1,)),\n",
        "      'word_inconclusion_presence': tf.keras.Input(shape=(1,)),\n",
        "      'repetitive_words': tf.keras.Input(shape=(1,)),\n",
        "      'Sentiment': tf.keras.Input(shape=(1,))\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_WmyIpKCWog"
      },
      "outputs": [],
      "source": [
        "#@title  Define functions that build and train the model\n",
        "def create_model(my_inputs, my_learning_rate, METRICS):\n",
        "  # Use a Concatenate layer to concatenate the input layers into a single tensor.\n",
        "  # as input for the Dense layer. Ex: [input_1[0][0], input_2[0][0]]\n",
        "  concatenated_inputs = tf.keras.layers.Concatenate()(my_inputs.values())\n",
        "\n",
        "  # activation set to sigmoid for binary classification models\n",
        "  dense = layers.Dense(units=1, name='dense_layer', activation=tf.sigmoid)\n",
        "  dense_output = dense(concatenated_inputs)\n",
        "\n",
        "  \"\"\" Create and compile a simple classification model. \"\"\"\n",
        "  my_outputs = {\n",
        "      'dense': dense_output,\n",
        "  }\n",
        "  model = tf.keras.Model(inputs=my_inputs, outputs=my_outputs)\n",
        "\n",
        "  # call the compile method to construct the layers into a model that\n",
        "  # Tensorflow can execute. Notice that we are using a different loss function\n",
        "  # for classification than for regression(BinaryCrossentropy())\n",
        "  # METRIC = is used to asses how well the model is performing on the training\n",
        "  # and validation data\n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=my_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=METRICS)\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(model, dataset, epochs, label_name,\n",
        "                batch_size=None, shuffle=True, validation_split=0.1):\n",
        "  \"\"\" Feed a dataset into the model in order to train it.\"\"\"\n",
        "  # The x parameter of tf.keras.Model.fit can be a list of arrays, where\n",
        "  # each array contains the data for one feature . Here, we are passing\n",
        "  # every column in the dataset. Note that the feature_layer will filter\n",
        "  # away most of those columns, leaving only the desired colums and their\n",
        "  # representations as features.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name))\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=shuffle, validation_split=validation_split)\n",
        "\n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "\n",
        "  #Isolate the classification metric for each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  return epochs, hist\n",
        "\n",
        "  print(\"Defined the create and train_model functions.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1XQQl24Cr6g"
      },
      "outputs": [],
      "source": [
        "# @title Define the plotting function\n",
        "def  plot_curve(epochs, hist, list_of_metrics):\n",
        "  \"\"\"plot a curve of one or more classification metrics vs. epoch.\"\"\"\n",
        "  #list_of_metrics should be one of the names shown in:\n",
        "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Value\")\n",
        "\n",
        "  for m in list_of_metrics:\n",
        "    x = hist[m]\n",
        "    plt.plot(epochs[1:], x[1:], label=m)\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "print(\"Defined the plot_curve function.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFZ_OZikDFTi"
      },
      "outputs": [],
      "source": [
        "#@title Invoke the creating, training and plotting functions\n",
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.001\n",
        "epochs = 25\n",
        "batch_size = 64\n",
        "label_name = \"LABEL\"\n",
        "classification_threshold = 0.50\n",
        "\n",
        "# Establish the metrics the model will measure.\n",
        "METRICS = [\n",
        "           tf.keras.metrics.BinaryAccuracy(name='accuracy',\n",
        "                                    threshold=classification_threshold),\n",
        "           # Add precision and recall as metrics\n",
        "           tf.keras.metrics.Precision(thresholds=classification_threshold,\n",
        "                                      name='precision'),\n",
        "\n",
        "           tf.keras.metrics.Recall(thresholds=classification_threshold,\n",
        "                                      name='recall'),\n",
        "           #summarize the model's perfomance\n",
        "           tf.keras.metrics.AUC(num_thresholds=100, name='auc')\n",
        "\n",
        "           ]\n",
        "# Remove precision, recall and auc from the METRICS list\n",
        "METRICS = [metric for metric in METRICS if 'val_' not in metric.name]\n",
        "\n",
        "# Establish the models' topography.\n",
        "my_model = create_model(inputs, learning_rate, METRICS)\n",
        "\n",
        "# To view a PNG of this model's layers click File>'my_classification_model.png'\n",
        "# file\n",
        "tf.keras.utils.plot_model(my_model, \"my_classification_model.png\")\n",
        "\n",
        "# Train the model on the training set.\n",
        "epochs, hist = train_model(my_model, train_df_norm_with_label, epochs,\n",
        "                           label_name, batch_size)\n",
        "\n",
        "# Plot a graph of the metric(s) vs. epochs.\n",
        "list_of_metrics_to_plot = ['accuracy', 'precision', 'recall']\n",
        "\n",
        "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
        "#Accuracy should gradually improve during training\n",
        "#(untill it can improve no more)\n",
        "\n",
        "#plotting the auc curve(metrics vs epoch)\n",
        "list_of_metric_to_plot2 = ['auc']\n",
        "plot_curve(epochs, hist, list_of_metric_to_plot2)\n",
        "\n",
        "my_model.save('/content/drive/MyDrive/fantasy.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onh2dd-zhKzU"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate the model against the test set\n",
        "# Determine the model's accuracy against the test set.\n",
        "\n",
        "features = {name:np.array(value) for name, value in test_df_norm_with_label.items()}\n",
        "label = np.array(features.pop(label_name))\n",
        "\n",
        "my_model.evaluate(x = features, y = label, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-DxQMl4Z8AY"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "# Sample input text\n",
        "input_text = \"By the end of the 25 epochs, the model has achieved a high accuracy, precision, recall, and AUC, and a low loss on both the training and validation sets. The high recall (1.0000) indicates that the model is correctly identifying all positive instances, while the high precision (0.8780) indicates that the majority of instances classified as positive are indeed positive. The high AUC (0.9627) indicates that the model is performing well in distinguishing between positive and negative instances.\"\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = re.split(r'\\W+', text.lower())\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in ENGLISH_STOP_WORDS]\n",
        "    # Rejoin the tokens into a single string\n",
        "    processed_text = ' '.join(tokens)\n",
        "    return processed_text\n",
        "\n",
        "input_text = preprocess_text(input_text)\n",
        "\n",
        "# Calculate the features for the input text\n",
        "def count_total_words(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    total_word_count = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        sentences = re.split(r'[.?!+]', paragraph)\n",
        "        for sentence in sentences:\n",
        "            words = sentence.split()\n",
        "            total_word_count += len(words)\n",
        "\n",
        "    return total_word_count\n",
        "\n",
        "def detect_total_numbers(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    has_number = any(any(char.isdigit() for char in paragraph) for paragraph in paragraphs)\n",
        "\n",
        "    return 1 if has_number else 0\n",
        "\n",
        "def detect_word_but_total(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    has_word_but = any('but' in paragraph.lower() for paragraph in paragraphs)\n",
        "    return 1 if has_word_but else 0\n",
        "\n",
        "def detect_word_in_conclusion_total(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    has_word_in_conclusion = any('conclusion' in paragraph.lower() for paragraph in paragraphs)\n",
        "    return 1 if has_word_in_conclusion else 0\n",
        "\n",
        "def detect_repetitive_language(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    freq_dist = nltk.FreqDist(tokens)\n",
        "\n",
        "    # Calculate the frequency of the most common words\n",
        "    most_common_words = sorted(freq_dist, key=freq_dist.get, reverse=True)[:10]\n",
        "    num_common_words = sum(freq_dist[word] > 5 for word in most_common_words)\n",
        "\n",
        "    if num_common_words > 5:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download VADER lexicon.\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    # Initialize VADER sentiment analyzer\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Calculate compound sentiment score for the entire text\n",
        "    sentiment_score = sia.polarity_scores(text)['compound']\n",
        "\n",
        "    # Classify sentiment based on the compound score\n",
        "    if sentiment_score >= 0.05:\n",
        "        sentiment = 1 # for positive\n",
        "    elif sentiment_score <= -0.05:\n",
        "        sentiment = -1 # for negative\n",
        "    else:\n",
        "        sentiment = 0\n",
        "\n",
        "    return sentiment\n",
        "\n",
        "features = {\n",
        "    'total_number_words': count_total_words(input_text),\n",
        "    'numbers_presence': detect_total_numbers(input_text),\n",
        "    'word_but_presence': detect_total_apostrophe(input_text),\n",
        "    'word_inconclusion_presence': detect_word_in_conclusion_total(input_text),\n",
        "    'repetitive_words': detect_repetitive_language(input_text),\n",
        "    'Sentiment': analyze_sentiment(input_text)\n",
        "}\n",
        "\n",
        "# Define the names of the features\n",
        "features_names = ['total_number_words', 'numbers_presence', 'word_but_presence', 'word_inconclusion_presence', 'repetitive_words', 'Sentiment']\n",
        "\n",
        "# Normalize the feature values using the mean and standard deviation from the training set\n",
        "train_df_mean = [316.2, 0.4, 0.5, 0.6, 0.7, 0.9]\n",
        "train_df_std = [77.4, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
        "features = (np.array(list(features.values())).reshape(1, -1) - train_df_mean) / train_df_std\n",
        "\n",
        "# Convert the NumPy array back to a dictionary\n",
        "features_dict = {name: value for name, value in zip(features_names, features[0])}\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/fantasy.h5')\n",
        "\n",
        "# Create a dictionary of input tensors\n",
        "input_tensors = {name:tf.constant(value.reshape(1,), dtype=tf.float32) for name, value in features_dict.items()}\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(input_tensors)\n",
        "\n",
        "for name, value in predictions.items():\n",
        "    print(f\"{name}: {value.shape}\")\n",
        "\n",
        "# Extract the prediction probablity\n",
        "probability = predictions['dense'][0][0]\n",
        "# Define the probability threshold\n",
        "\n",
        "#threshold = 0.5\n",
        "\n",
        "# Print the classification result\n",
        "if probability > classification_threshold:\n",
        "    print(\"The text is likely AI-generated.\")\n",
        "else:\n",
        "    print(\"The text is likely human-generated.\")\n",
        "\n",
        "print(probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9XfnxlyWGsd"
      },
      "outputs": [],
      "source": [
        "# @title saving a pickle file to use for deployment\n",
        "import pickle\n",
        "\n",
        "# Save the model as a pickle file\n",
        "with open('model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1082ZhubpBEy7EaB2XR047KCe5ORS8bLt",
      "authorship_tag": "ABX9TyOTerqPieDVjPPZT5le5+sm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}